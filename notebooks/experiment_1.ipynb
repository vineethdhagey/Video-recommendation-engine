{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52aa122e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define the expected path for the external slang dictionary file\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#SLANG_FILE_PATH = os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'slangs.json')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Project root (assuming this script is anywhere inside the project)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m PROJECT_ROOT \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (PROJECT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     13\u001b[0m     PROJECT_ROOT \u001b[38;5;241m=\u001b[39m PROJECT_ROOT\u001b[38;5;241m.\u001b[39mparent\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Define the expected path for the external slang dictionary file\n",
    "#SLANG_FILE_PATH = os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'slangs.json')\n",
    "# Project root (assuming this script is anywhere inside the project)\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "while not (PROJECT_ROOT / \"data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "# Constant path to your JSON\n",
    "SLANG_FILE_PATH = PROJECT_ROOT / \"data\" / \"slangs.json\"\n",
    "\n",
    "def _load_slang_map() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Attempts to load the slang map from the JSON file path.\n",
    "    Falls back to a hardcoded dictionary if the file is not found or fails to load.\n",
    "    \"\"\"\n",
    "    # Hardcoded fallback dictionary (development/testing)\n",
    "    FALLBACK_SLANG_MAP: Dict[str, str] = {\n",
    "        \"lol\": \"laughing out loud\",\n",
    "        \"brb\": \"be right back\",\n",
    "        \"btw\": \"by the way\",\n",
    "        \"imo\": \"in my opinion\",\n",
    "        \"imho\": \"in my humble opinion\",\n",
    "        \"thx\": \"thanks\",\n",
    "        \"ty\": \"thank you\",\n",
    "        \"tldr\": \"too long didn't read\",\n",
    "        \"ikr\": \"i know, right\",\n",
    "        \"fomo\": \"fear of missing out\",\n",
    "        \"smh\": \"shaking my head\",\n",
    "        #\"omg\": \"oh my god\",\n",
    "        \"wtf\": \"what the hell\",\n",
    "        \"lmao\": \"laughing my a** off\",\n",
    "        \"np\": \"no problem\",\n",
    "        \"afk\": \"away from keyboard\",\n",
    "        \"gg\": \"good game\",\n",
    "        \"gtg\": \"got to go\",\n",
    "        \"rofl\": \"rolling on the floor laughing\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load the external JSON file\n",
    "        with open(SLANG_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            print(f\"INFO: Successfully loaded slang map from {SLANG_FILE_PATH}\")\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"WARNING: Slang file not found at {SLANG_FILE_PATH}. Using hardcoded fallback map.\")\n",
    "        return FALLBACK_SLANG_MAP\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ERROR: Failed to decode JSON from {SLANG_FILE_PATH}. Using hardcoded fallback map.\")\n",
    "        return FALLBACK_SLANG_MAP\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during slang map loading: {e}. Using hardcoded fallback map.\")\n",
    "        return FALLBACK_SLANG_MAP\n",
    "\n",
    "# Load the slang map once when the module is imported\n",
    "SLANG_MAP = _load_slang_map()\n",
    "\n",
    "def expand_slang_and_abbreviations(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardizes text by expanding common internet slang and abbreviations \n",
    "    based on the loaded map. This function is case-insensitive during matching.\n",
    "\n",
    "    Args:\n",
    "        text: The input string (e.g., \"This is awesome lol btw I'm late\").\n",
    "\n",
    "    Returns:\n",
    "        The string with slang expanded (e.g., \"This is awesome laughing out loud by the way I'm late\").\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # 1. Lowercase the entire text and split into words\n",
    "    words: List[str] = text.lower().split()\n",
    "    \n",
    "    # 2. Process words and replace slang\n",
    "    expanded_words: List[str] = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Check if the word, after stripping any trailing punctuation, is in the slang map.\n",
    "        # We need to preserve punctuation (like periods or commas) if they are attached \n",
    "        # to the slang, but we match the core word.\n",
    "        clean_word = re.sub(r'[\\W_]+$', '', word) # Remove trailing non-word characters\n",
    "        \n",
    "        if clean_word in SLANG_MAP:\n",
    "            # Replace the word with the expanded version\n",
    "            expanded_slang = SLANG_MAP[clean_word]\n",
    "            \n",
    "            # If the original word had trailing punctuation (e.g., \"lol.\"), \n",
    "            # we re-attach it to the expanded phrase.\n",
    "            trailing_punc = re.search(r'([\\W_]+$)', word)\n",
    "            if trailing_punc:\n",
    "                # We need to correctly handle multi-word expansion (e.g., \"laughing out loud\")\n",
    "                # and re-attach punctuation to the last word of the expanded phrase.\n",
    "                expanded_slang_list = expanded_slang.split()\n",
    "                if expanded_slang_list:\n",
    "                    expanded_slang_list[-1] += trailing_punc.group(1)\n",
    "                expanded_words.extend(expanded_slang_list)\n",
    "            else:\n",
    "                expanded_words.extend(expanded_slang.split())\n",
    "        else:\n",
    "            expanded_words.append(word)\n",
    "\n",
    "    return ' '.join(expanded_words)\n",
    "\n",
    "# Example Usage to test the function\n",
    "if __name__ == \"__main__\":\n",
    "    test_comment_1 = \"OMG this is the best tutorial ever lol! IKR?\"\n",
    "    test_comment_2 = \"BTW, I think this is too long. TLDR. SMH.\"\n",
    "    test_comment_3 = \"thx for the help. np.\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(f\"Original 1: {test_comment_1}\")\n",
    "    print(f\"Expanded 1: {expand_slang_and_abbreviations(test_comment_1)}\\n\")\n",
    "    \n",
    "    print(f\"Original 2: {test_comment_2}\")\n",
    "    print(f\"Expanded 2: {expand_slang_and_abbreviations(test_comment_2)}\\n\")\n",
    "\n",
    "    print(f\"Original 3: {test_comment_3}\")\n",
    "    print(f\"Expanded 3: {expand_slang_and_abbreviations(test_comment_3)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10efa6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh my god\n",
      "None\n",
      "oh my god\n",
      "Total entries in slang JSON: 2886\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "file_path = os.path.join(\"..\", \"data\", \"slang_words_normalized.json\")\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    slang_dict = json.load(f)\n",
    "\n",
    "# Check a few slangs\n",
    "print(slang_dict.get(\"omg\"))   # Example: Check \"OMG\"\n",
    "print(slang_dict.get(\"BTW\"))   # Example: Check \"BRB\"\n",
    "print(slang_dict.get(\"omg\"))   # Example: Check \"LOL\"\n",
    "num_entries = len(slang_dict)\n",
    "print(f\"Total entries in slang JSON: {num_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d850ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hemanthanne/Desktop/SVRE/svre-env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/hemanthanne/Desktop/SVRE/svre-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/hemanthanne/Desktop/SVRE/svre-env/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "# Model name for multi-language ‚Üí English\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-mul-en\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = MarianTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = MarianMTModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Use GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9b3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_offline(sentences, batch_size=8):\n",
    "    \"\"\"\n",
    "    Translate a list of sentences to English using MarianMT offline.\n",
    "    \"\"\"\n",
    "    translations = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        # Generate translation IDs\n",
    "        translated = model.generate(**inputs)\n",
    "        # Decode translation\n",
    "        translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "        translations.extend(translated_texts)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c7783f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Bonjour, comment √ßa va ?\n",
      "Translated: Hello, how's it going?\n",
      "\n",
      "Original: Hola amigo, ¬øc√≥mo est√°s?\n",
      "Translated: Hey, buddy, how are you?\n",
      "\n",
      "Original: ‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\n",
      "Translated: It's very good.\n",
      "\n",
      "Original: Dies ist ein Test\n",
      "Translated: This is a test\n",
      "\n",
      "Original: Ciao, come stai?\n",
      "Translated: Hey, how are you?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"Bonjour, comment √ßa va ?\",        # French\n",
    "    \"Hola amigo, ¬øc√≥mo est√°s?\",        # Spanish\n",
    "    \"‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\",                 # Hindi\n",
    "    \"Dies ist ein Test\",               # German\n",
    "    \"Ciao, come stai?\",               # Italian\n",
    "]\n",
    "\n",
    "translated = translate_offline(examples)\n",
    "for original, trans in zip(examples, translated):\n",
    "    print(f\"Original: {original}\\nTranslated: {trans}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40f0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both CSV files\n",
    "df1 = pd.read_csv(\"C:/Users/Vineeth/Desktop/SVRE_P1/data/raw/comments.csv\")\n",
    "df2 = pd.read_csv(\"C:/Users/Vineeth/Desktop/SVRE_P1/data/raw/raw_comments.csv\")\n",
    "\n",
    "# Concatenate row-wise\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save to a new CSV\n",
    "df_combined.to_csv(\"C:/Users/Vineeth/Desktop/SVRE_P1/data/processed/all_comments_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68574789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "GPU Memory: 4.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "# ---------------------- Configuration ----------------------\n",
    "MODEL_PATH = \"C:\\Users\\Vineeth\\Desktop\\SVRE_P1\\models\\distillbert\"  # Path to your saved model folder\n",
    "device = torch.device(\"cpu\")  # Use CPU for inference\n",
    "\n",
    "# ---------------------- Load model and tokenizer ----------------------\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# ---------------------- Label mappings ----------------------\n",
    "# Make sure these match the mapping used during training\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# ---------------------- Test sentences ----------------------\n",
    "sentences = [\n",
    "    \"I love this product! It's amazing.\",\n",
    "    \"The movie was okay, not great but not bad either.\",\n",
    "    \"This is the worst service I have ever received.\"\n",
    "]\n",
    "\n",
    "# ---------------------- Tokenization ----------------------\n",
    "inputs = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Move inputs to the correct device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# ---------------------- Inference ----------------------\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_indices = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "# Map predicted indices back to labels\n",
    "predicted_labels = [id2label[idx] for idx in predicted_indices]\n",
    "\n",
    "# ---------------------- Display results ----------------------\n",
    "for sentence, label in zip(sentences, predicted_labels):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Predicted Sentiment: {label}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c24d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vineeth\\Desktop\\SVRE_P1\\svre-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label: {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'}\n",
      "label2id: {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model_path = r\"C:\\Users\\Vineeth\\Desktop\\SVRE_P1\\models\\distillbert\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "print(\"id2label:\", model.config.id2label)\n",
    "print(\"label2id:\", model.config.label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94762c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this product! ‚Üí 2\n",
      "This is terrible. ‚Üí 0\n",
      "It's okay, not bad. ‚Üí 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "import torch\n",
    "\n",
    "model_path = r\"C:\\Users\\Vineeth\\Desktop\\SVRE_P1\\models\\distillbert\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_sentences = [\n",
    "    \"I love this product!\",    # positive\n",
    "    \"This is terrible.\",       # negative\n",
    "    \"It's okay, not bad.\"      # neutral\n",
    "]\n",
    "\n",
    "inputs = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "for s, p in zip(test_sentences, preds):\n",
    "    print(f\"{s} ‚Üí {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869619ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: I/O error: The requested operation cannot be performed on a file with a user-mapped section open. (os error 1224)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlabel2id \u001b[38;5;241m=\u001b[39m label2id\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Save updated model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Model labels updated to -1, 0, 1 and saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Vineeth\\Desktop\\SVRE_P1\\svre-env\\lib\\site-packages\\transformers\\modeling_utils.py:4177\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[0;32m   4172\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m   4174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[0;32m   4175\u001b[0m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[0;32m   4176\u001b[0m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[1;32m-> 4177\u001b[0m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4179\u001b[0m     save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[1;32mc:\\Users\\Vineeth\\Desktop\\SVRE_P1\\svre-env\\lib\\site-packages\\safetensors\\torch.py:352\u001b[0m, in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_file\u001b[39m(\n\u001b[0;32m    322\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    323\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    324\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ):\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 352\u001b[0m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSafetensorError\u001b[0m: Error while serializing: I/O error: The requested operation cannot be performed on a file with a user-mapped section open. (os error 1224)"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model_path = r\"C:\\Users\\Vineeth\\Desktop\\SVRE_P1\\models\\distillbert\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Map old labels to numeric scores directly\n",
    "id2label = {0: -1, 1: 0, 2: 1}  # 0->Negative(-1), 1->Neutral(0), 2->Positive(1)\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Update model config\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "# Save updated model\n",
    "model.save_pretrained(model_path)\n",
    "print(\"‚úÖ Model labels updated to -1, 0, 1 and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c5130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this product! ‚Üí 1\n",
      "This is terrible. ‚Üí -1\n",
      "It's okay, not bad. ‚Üí 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = r\"C:\\Users\\Vineeth\\Desktop\\SVRE_P1\\models\\distillbert\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"I love this product!\",    # positive\n",
    "    \"This is terrible.\",       # negative\n",
    "    \"It's okay, not bad.\"      # neutral\n",
    "]\n",
    "\n",
    "# Tokenize inputs\n",
    "inputs = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Map model IDs to -1, 0, 1\n",
    "id2score = {0: -1, 1: 0, 2: 1}\n",
    "\n",
    "# Print results\n",
    "for s, p in zip(test_sentences, preds):\n",
    "    score = id2score[p]\n",
    "    print(f\"{s} ‚Üí {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a500ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: excellent tutorial the visuals were great\n",
      "Predicted Label: Positive, Score: 1\n",
      "\n",
      "Comment: i loved spending an hour just to get to the main point brilliant pacing\n",
      "Predicted Label: Positive, Score: 1\n",
      "\n",
      "Comment: oh yes very clear i totally understand how that works now what a masterpiece\n",
      "Predicted Label: Positive, Score: 1\n",
      "\n",
      "Comment: the instructor explained the formula clearly and effectively\n",
      "Predicted Label: Positive, Score: 1\n",
      "\n",
      "Comment: i feel much more confident about the topic now\n",
      "Predicted Label: Positive, Score: 1\n",
      "\n",
      "Comment: a solid explanation of a complex subject\n",
      "Predicted Label: Neutral, Score: 0\n",
      "\n",
      "Comment: the content was a bit advanced but still very useful\n",
      "Predicted Label: Neutral, Score: 0\n",
      "\n",
      "Comment: i appreciate the effort put into this lesson\n",
      "Predicted Label: Positive, Score: 1\n",
      "\n",
      "Comment: my brain cells are having a party of pure confusion this is top-tier education\n",
      "Predicted Label: Positive, Score: 1\n",
      "\n",
      "Comment: the audio quality is so bad i can barely hear what the instructor is saying\n",
      "Predicted Label: Negative, Score: -1\n",
      "\n",
      "Comment: the presenter just reads off the slides this isnt teaching\n",
      "Predicted Label: Negative, Score: -1\n",
      "\n",
      "Comment: i feel like i know less now than i did before watching this confusing mess\n",
      "Predicted Label: Negative, Score: -1\n",
      "\n",
      "Comment: it was just another video in the series nothing unexpected\n",
      "Predicted Label: Neutral, Score: 0\n",
      "\n",
      "Comment: this is a suitable introductory video for people unfamiliar with the concept\n",
      "Predicted Label: Neutral, Score: 0\n",
      "\n",
      "Comment: the facts presented seem to be accurate based on what i already know\n",
      "Predicted Label: Neutral, Score: 0\n",
      "\n",
      "Overall sentiment score: 4\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- Load model and tokenizer --------------------\n",
    "model_path = r\"C:\\Users\\Vineeth\\Desktop\\SVRE_P1\\models\\distillbert_finetuned_v2\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -------------------- Load inference data --------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\Vineeth\\Desktop\\SVRE_P1\\data\\inference\\inference_new.csv\")  # CSV must have a column 'Comment'\n",
    "comments = df['text'].tolist()\n",
    "\n",
    "# -------------------- Tokenize and predict --------------------\n",
    "inputs = tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# -------------------- Map model IDs to sentiment scores --------------------\n",
    "id2score = {0: -1, 1: 0, 2: 1}\n",
    "scores = [id2score[p] for p in preds]\n",
    "\n",
    "# -------------------- Add predictions to DataFrame --------------------\n",
    "df['Predicted_Score'] = scores\n",
    "df['Predicted_Label'] = ['Negative' if s==-1 else 'Neutral' if s==0 else 'Positive' for s in scores]\n",
    "\n",
    "# -------------------- Calculate overall score --------------------\n",
    "overall_score = sum(scores)\n",
    "\n",
    "# -------------------- Save results --------------------\n",
    "output_path = r\"C:\\Users\\Vineeth\\Desktop\\SVRE_P1\\data\\inference_results2.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# -------------------- Print results --------------------\n",
    "for comment, label, score in zip(df['text'], df['Predicted_Label'], df['Predicted_Score']):\n",
    "    print(f\"Comment: {comment}\\nPredicted Label: {label}, Score: {score}\\n\")\n",
    "\n",
    "print(f\"Overall sentiment score: {overall_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57131d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Using cached google_api_python_client-2.184.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from google-api-python-client) (0.31.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from google-api-python-client) (2.41.1)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 (from google-api-python-client)\n",
      "  Using cached google_api_core-2.26.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from google-api-python-client) (4.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (6.32.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.5)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.10.5)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vineeth\\desktop\\svre_p1\\svre-env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Using cached google_api_python_client-2.184.0-py3-none-any.whl (14.3 MB)\n",
      "Using cached google_api_core-2.26.0-py3-none-any.whl (162 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: google-auth-httplib2, google-api-core, google-api-python-client\n",
      "\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   ------------- -------------------------- 1/3 [google-api-core]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   -------------------------- ------------- 2/3 [google-api-python-client]\n",
      "   ---------------------------------------- 3/3 [google-api-python-client]\n",
      "\n",
      "Successfully installed google-api-core-2.26.0 google-api-python-client-2.184.0 google-auth-httplib2-0.2.0\n"
     ]
    }
   ],
   "source": [
    "# üß© Step 1: Install dependencies (if not already)\n",
    "!pip install google-api-python-client tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffde388f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DistilBertForSequenceClassification, DistilBertTokenizerFast\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_preprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextPreprocessor\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "import torch\n",
    "from src.components.data_preprocessing import TextPreprocessor\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f00028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© Step 3: Set up YouTube API\n",
    "API_KEY = \"AIzaSyCoz9NrmBu5mFRm_-qD4XoTFaqu7AGvGeU\"  # üîπ Replace with your YouTube API key\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# Function to extract video ID from a full YouTube URL\n",
    "def extract_video_id(url):\n",
    "    import re\n",
    "    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab933c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© Step 4: Fetch YouTube comments\n",
    "def fetch_youtube_comments(video_url, max_comments=100):\n",
    "    video_id = extract_video_id(video_url)\n",
    "    comments = []\n",
    "    \n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    while response:\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comments.append(comment)\n",
    "            if len(comments) >= max_comments:\n",
    "                return comments\n",
    "        \n",
    "        if \"nextPageToken\" in response:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                pageToken=response[\"nextPageToken\"],\n",
    "                maxResults=100,\n",
    "                textFormat=\"plainText\"\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "    return comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c06e3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fetched 12 comments.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy teachers day sir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thanku sir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you sir for ur easy explanation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thanku so much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is pop and structured programing same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>üòçüòçüòçplease do make these videos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Plz explain us concept of oop in c++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sir i learned this from u r class only it is v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nice sir tq for explaining the opp and pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sir this video is super good to know differenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0                             happy teachers day sir\n",
       "1                                         Thanku sir\n",
       "2              Thank you sir for ur easy explanation\n",
       "3                                     Thanku so much\n",
       "4              Is pop and structured programing same\n",
       "5                    üòçüòçüòçplease do make these videos.\n",
       "6               Plz explain us concept of oop in c++\n",
       "7  Sir i learned this from u r class only it is v...\n",
       "8         Nice sir tq for explaining the opp and pop\n",
       "9  Sir this video is super good to know differenc..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üß© Step 5: Test comment fetching\n",
    "video_url = \"https://www.youtube.com/watch?v=liBWJp2OfUU&list=PLXj4XH7LcRfDlQklXu3Hrtru-bm2dJ9Df&index=2\"  # üîπ Replace with an educational video link\n",
    "comments = fetch_youtube_comments(video_url, max_comments=200)\n",
    "print(f\"‚úÖ Fetched {len(comments)} comments.\")\n",
    "pd.DataFrame(comments, columns=[\"comment\"]).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592ad5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© Step 6: Load your fine-tuned model\n",
    "model_path = r\"C:\\Users\\Vineeth\\Desktop\\SVRE_P1\\models\\distillbert_finetuned_v2\"\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Label mapping\n",
    "id2label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "id2score = {0: -1, 1: 0, 2: 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d0aad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© Step 7: Predict sentiment for comments\n",
    "def predict_sentiment(comments):\n",
    "    results = []\n",
    "    batch_size = 16  # Adjust for GPU memory\n",
    "    \n",
    "    for i in tqdm(range(0, len(comments), batch_size)):\n",
    "        batch = comments[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "        for text, pred in zip(batch, preds):\n",
    "            results.append({\n",
    "                \"comment\": text,\n",
    "                \"pred_label\": id2label[pred],\n",
    "                \"score\": id2score[pred]\n",
    "            })\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaedd52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy teachers day sir</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thanku sir</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you sir for ur easy explanation</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thanku so much</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is pop and structured programing same</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>üòçüòçüòçplease do make these videos.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Plz explain us concept of oop in c++</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sir i learned this from u r class only it is v...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nice sir tq for explaining the opp and pop</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sir this video is super good to know differenc...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment pred_label  score\n",
       "0                             happy teachers day sir    Neutral      0\n",
       "1                                         Thanku sir   Positive      1\n",
       "2              Thank you sir for ur easy explanation   Positive      1\n",
       "3                                     Thanku so much   Positive      1\n",
       "4              Is pop and structured programing same    Neutral      0\n",
       "5                    üòçüòçüòçplease do make these videos.    Neutral      0\n",
       "6               Plz explain us concept of oop in c++    Neutral      0\n",
       "7  Sir i learned this from u r class only it is v...   Positive      1\n",
       "8         Nice sir tq for explaining the opp and pop    Neutral      0\n",
       "9  Sir this video is super good to know differenc...    Neutral      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üß© Step 8: Run inference on fetched comments\n",
    "df_results = predict_sentiment(comments)\n",
    "df_results.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82531d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© Step 9: Calculate overall sentiment score\n",
    "overall_score = df_results[\"score\"].sum()\n",
    "print(f\"üéØ Overall sentiment score: {overall_score}\")\n",
    "\n",
    "# Show 10 random predictions\n",
    "sample_predictions = df_results.sample(10, random_state=42)\n",
    "sample_predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
